[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "first_test.html",
    "href": "first_test.html",
    "title": "Function to convert corners to bounding box and bouding box to corners",
    "section": "",
    "text": "import sys\nfrom pathlib import Path\n\nfrom miniai.activations import set_seed\nfrom miniai.datasets import show_image\nfrom miniai.learner import *\n\nfrom fastai.vision.all import L,imagenet_stats,create_body,resnet34\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom IPython.display import HTML\nimport cv2\n\nimport torch\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\n#from torch.utils.data.dataloader import default_collate\nimport torch.nn.functional as F\n\nfrom torchvision import transforms as tfms\nfrom torchvision.datasets import CocoDetection\n\nfrom imgaug.augmentables.bbs import BoundingBoxesOnImage\nfrom imgaug import augmenters as iaa \n\nIN_COLAB = 'google.colab' in sys.modules\n\ndevice = torch.device('cuda',0) if torch.cuda.is_available() else torch.device('cpu')\nset_seed(6789, deterministic=False)\n\nSIZE = 224  # size of images we'll us\n\n\ndata_path = \"./coco_2017\" # Path to where you want to store the data\nINFERENCE_ONLY = False # Set to True to skip downloading training data\ndef download_coco():\n    print('Coco Download Started.')\n    !mkdir -p {data_path}\n    !curl \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\" &gt; {data_path}/annotations_trainval2017.zip\n    !unzip -q -o {data_path}/annotations_trainval2017.zip -d {data_path}\n    !rm {data_path}/annotations_trainval2017.zip\n    !curl \"http://images.cocodataset.org/zips/val2017.zip\" &gt; {data_path}/val2017.zip\n    !unzip -q -o {data_path}/val2017.zip -d {data_path}\n    !rm {data_path}/val2017.zip\n    if not INFERENCE_ONLY:\n        !curl \"http://images.cocodataset.org/zips/train2017.zip\" &gt; {data_path}/train2017.zip\n        !unzip -q -o {data_path}/train2017.zip -d {data_path}\n        !rm {data_path}/train2017.zip\n    else: print('Warning: Inference Only Skipping download of training dataset')\n    print('Coco Download Completed.')\n\ndownload_coco()\n\nCoco Download Started.\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  241M  100  241M    0     0  5888k      0  0:00:41  0:00:41 --:--:-- 5849k   0  0:00:49  0:00:04  0:00:45 5032k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  777M  100  777M    0     0  4608k      0  0:02:52  0:02:52  0:00:01 6095k2:36  0:00:22 5979k:02:52 --:--:-- 5932k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 18.0G  100 18.0G    0     0  4763k      0  1:06:04  1:06:04 --:--:-- 6051k5:06:26 1750k 4616k      0  1:08:10  0:00:40  1:07:30 5934k21 4157k4561k      0  1:08:59  0:05:33  1:03:26 4171k   0  1:07:54  0:05:47  1:02:07 6364k52M    0     0  4697k      0  1:06:59  0:07:05  0:59:54 5141k8  0:07:20  0:58:58 6138k   0  1:05:20  0:08:08  0:57:12 4681k:19  0:10:49  0:52:30 5928k2  0:51:05 5781k     0  4984k      0  1:03:08  0:11:53  0:51:15 2955k   0  4960k      0  1:03:26  0:12:08  0:51:18 4357kk      0  1:03:10  0:12:35  0:50:35 5777k053k      0  1:02:16  0:16:36  0:45:40 1079k2570k 4970k      0  1:03:19  0:20:29  0:42:50 2648k      0  1:03:49  0:20:44  0:43:05 1757k 0     0  5001k      0  1:02:55  0:26:59  0:35:56 4478k5060k      0  1:02:11  0:29:35  0:32:36 6088k  0:29:43  0:32:25 5827kM    0     0  4967k      0  1:03:21  0:32:00  0:31:21 2347k      0  1:03:37  0:33:37  0:30:00 1473k:29:24 4711kk 1:05:03  0:38:23  0:26:40 3200k613k 1:05:12  0:43:26  0:21:46 4111k 0  4816k      0  1:05:20  0:46:03  0:19:17 4626k0  4817k      0  1:05:19  0:46:58  0:18:21 2812k3G    0     0  4815k      0  1:05:21  0:48:20  0:17:01 4323k    0  4810k      0  1:05:25  0:49:52  0:15:33 3868k14k      0  1:05:22  0:54:08  0:11:14 6644k  0  4820k      0  1:05:17  0:55:21  0:09:56 6099k06k  0  4812k      0  1:05:23  0:58:59  0:06:24 6183k:05:04  1:01:02  0:04:02 3650k:05:13  1:01:56  0:03:17 5489k0     0  4824k      0  1:05:14  1:02:12  0:03:02 2742k 0  4819k      0  1:05:18  1:02:17  0:03:01 1012k   0  4805k      0  1:05:29  1:02:29  0:03:00  654k6 17.3G    0     0  4795k      0  1:05:38  1:03:19  0:02:19 4943k  0  4768k      0  1:06:00  1:04:00  0:02:00 1029k.0G   97 17.4G    0     0  4754k      0  1:06:11  1:04:15  0:01:56  988k  97 17.5G    0     0  4753k      0  1:06:12  1:04:36  0:01:36 4284k1:11 4758k1:06:16  1:05:09  0:01:07 4777k\nCoco Download Completed.\n\n\n\ncolors = [None, (39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86),\n          (14, 89, 122),\n          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),\n          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),\n          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),\n          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),\n          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),\n          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),\n          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),\n          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),\n          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),\n          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),\n          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),\n          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),\n          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),\n          (2, 20, 184), (122, 37, 185)]\n\nvocab = [\"bg\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n                \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\",\n                \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n                \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n                \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n                \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\",\n                \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\",\n                \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n                \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n                \"teddy bear\", \"hair drier\", \"toothbrush\"]\n\ncat2id = {v:k for k,v in enumerate(vocab)}\nid2cat = {k:v for k,v in enumerate(vocab)}\n\nprint(f'The number of classes in the dataset: {len(vocab)}')\n\nThe number of classes in the dataset: 81\n\n\n\nxmean,xstd = L(imagenet_stats).map(tensor)\n\n\nxmean, xstd\n\n(tensor([0.4850, 0.4560, 0.4060]), tensor([0.2290, 0.2240, 0.2250]))\n\n\n\nnum_workers = 4\nbatch_size = 16\nnum_gpus = 1\n\n\nfrom torch.utils.data import DataLoader, default_collate\n\n\nex_t = torch.zeros(5,4)\n\n\nt_ = torch.zeros(1,4)\ntorch.cat([t_,ex_t],dim=0).shape\n\ntorch.Size([6, 4])\n\n\n\ndef pad_bb_tensor(t, m):\n    return torch.cat([t, torch.zeros(m - len(t), 4)], dim=0)\n\ndef pad_cl_tensor(t, m):\n    return torch.cat([t, torch.zeros(m - len(t))], dim=0).long()\n\n# We need to zero-pad the boxes and labels to the max number of boxes in a batch\ndef collate_fn(batch):\n    items = list(zip(*batch))\n    \n    m = 0\n    for i in items[1]:\n        if i is None:\n            continue\n        l = len(i) \n        if l &gt; m: m = l\n\n    items[0] = default_collate([i for i in items[0] if torch.is_tensor(i)])\n    boxes = [pad_bb_tensor(i,m) for i in items[1] if torch.is_tensor(i)]\n    clses = [pad_cl_tensor(i,m) for i in items[2] if torch.is_tensor(i)]\n\n    items[1] = default_collate(boxes)\n    items[2] = default_collate(clses)\n\n    return items\n\n\ntrain_params = {\"batch_size\": batch_size * num_gpus,\n                \"shuffle\": True,\n                \"drop_last\": False,\n                \"num_workers\": num_workers,\n                \"collate_fn\": collate_fn\n                }\n\n\nvalid_params = {\"batch_size\": batch_size * num_gpus,\n                \"shuffle\": False,\n                \"drop_last\": False,\n                \"num_workers\": num_workers,\n                \"collate_fn\": collate_fn\n                }\n\n\nclass CocoDataset(CocoDetection):\n    def __init__(\n                 self, \n                 root, \n                 year, \n                 mode, \n                 transform=None):\n        root = Path(root)\n        annFile = root/f\"annotations/instances_{mode}{year}.json\"\n        root = root/f\"{mode}{year}\"\n        super(CocoDataset, self).__init__(root, annFile)\n        self._load_categories()\n        self.transform = transform\n\n    def _load_categories(self):\n        categories = self.coco.loadCats(self.coco.getCatIds())\n        categories.sort(key=lambda x: x[\"id\"])\n\n        self.label_map = {}\n        self.label_info = {}\n        counter = 1\n        self.label_info[0] = \"background\"\n        for c in categories:\n            self.label_map[c[\"id\"]] = counter\n            self.label_info[counter] = c[\"name\"]\n            counter += 1\n\n    def __getitem__(self, item):\n        image, target = super(CocoDataset, self).__getitem__(item)\n        width, height = image.size\n        boxes = []\n        labels = []\n        if len(target) == 0:\n            return None, None, None, None, None\n        for annotation in target:\n            bbox = annotation.get(\"bbox\")\n            boxes.append([bbox[0], bbox[1], (bbox[0] + bbox[2]), (bbox[1] + bbox[3])])\n            labels.append(self.label_map[annotation.get(\"category_id\")])\n        if self.transform is not None:\n            image, (height, width), boxes, labels = self.transform(image, (height, width), boxes, labels)\n        return image, torch.tensor(boxes), torch.tensor(labels).long()\n\n\n# denormalize the image data based on imagenet stats\ndef denormalize(x):\n    return x*xstd[...,None,None]+xmean[...,None,None]\n\n# scale and center the image data based on imagenet stats\ndef normalize(x):\n    return (x-xmean[:,None,None])/xstd[:,None,None]\n    \nclass Xfm(object):\n    def __init__(self,size,augment=True):\n        self.augment = augment\n        self.size = size\n        self.tseq = iaa.Sequential([\n            iaa.Resize({\"height\": size[0], \"width\": size[1]}),\n            iaa.Fliplr(0.5),\n            iaa.Flipud(0.20),\n            iaa.GammaContrast(1.5),\n            iaa.Sometimes(\n                0.5,iaa.Sequential([iaa.Affine(translate_percent={\"x\": (-0.15,0.15),\"y\": (-0.15,0.15)},rotate=(5,-5), scale=(0.15, 1.5)),\n                iaa.CoarseDropout(0.1, per_channel=True)    \n            ]))\n        ])\n        self.vseq = iaa.Sequential([\n            iaa.Resize({\"height\": size[0], \"width\": size[1]})\n        ])\n\n    def __call__(\n                self,\n                img,\n                _img_size,\n                bboxes=None,\n                labels=None\n                ):\n        img = np.array(img)\n        bbs = BoundingBoxesOnImage.from_xyxy_array(bboxes, shape=img.shape)\n\n        if (self.augment): \n            n = len(list(bbs.bounding_boxes))\n\n            im_aug,bbs_aug = self.tseq(image=img, bounding_boxes=bbs)\n\n            bbs_aug = bbs_aug.clip_out_of_image()\n            n2 = len(list(bbs_aug.bounding_boxes))         \n            if n != n2: # at least one bounding box is fully outside of the image boundary\n                # bail out of augmentation for this sample\n                #print('bailing out of augmentation for this sample')\n                im_aug,bbs_aug = self.vseq(image=img, bounding_boxes=bbs)\n        else: im_aug,bbs_aug = self.vseq(image=img, bounding_boxes=bbs)\n\n        b = (bbs_aug.to_xyxy_array()/(self.size[0]/2))-1  # scale the bounding boxes to be between -1 and 1\n\n        img = normalize(tfms.ToTensor()(im_aug)) # normalize the image data with imagenet stats\n\n        return img,self.size,b,labels\n\n\ntrain_set = CocoDataset(data_path,2017,\"train\",Xfm((SIZE,SIZE),True))\n train_dl = DataLoader(train_set, **train_params)\n\nloading annotations into memory...\nDone (t=16.26s)\ncreating index...\nindex created!\n\n\n\nval_set = CocoDataset(data_path,2017,\"val\",Xfm((SIZE,SIZE),False))\n val_dl = DataLoader(val_set, **valid_params)\n\nloading annotations into memory...\nDone (t=0.92s)\ncreating index...\nindex created!\n\n\n\n# takes an image tensor; denormalizes it and scales it up so that we have enough pixels to render our annotations reasonably\ndef denormalize_tensor(im_tensor):\n    im = denormalize(im_tensor)\n    im = (im - im.min()/(im.max()-im.min())).clip(0,1)\n    im = np.array(im.permute(1,2,0)*255).astype(np.uint8).copy()\n    im = cv2.resize(im, (768,768))\n    return im\n\n\ndef showobjects(im,bbox,clas):\n    annotated_im = np.copy(im)\n\n    # composite annotations onto image\n    h,w = annotated_im.shape[:2]\n    # for each non-background class label\n    for i in range(len(clas)):\n        if clas[i] == 0: continue # skip background\n        category=id2cat[clas[i].item()]\n        color = colors[clas[i]]\n\n        box = (bbox[i]+1)/2\n        x1,y1,x2,y2 = box.tolist()\n        x1 = int(x1*w)\n        x2 = int(x2*w)\n        y1 = int(y1*h)\n        y2 = int(y2*h)\n\n        # draw bounding box\n        cv2.rectangle(annotated_im,(x1,y1),(x2,y2),color,2)\n\n        # draw category label\n        text_size = cv2.getTextSize(category, cv2.FONT_HERSHEY_PLAIN, 1.5, 1)[0]\n        cv2.rectangle(annotated_im, (x1, y1), (x1 + text_size[0] + 3, y1 + text_size[1] + 4), color,\n                        -1)\n        cv2.putText(annotated_im, category,\n            (x1, y1 + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1.5,\n            (255, 255, 255), 1)   \n    return annotated_im\n\n\nnum_to_show = 3\nfig,ax = plt.subplots(num_to_show,1,figsize=(7,7*num_to_show))\nfor i in range(num_to_show):\n    show_image(showobjects(denormalize_tensor(train_set[i][0]),train_set[i][1],train_set[i][2]),ax=ax[i])\nfig.tight_layout()\n_ = ax[0].set_title('Ground Truth')\n\n\n\n\n\nnum_to_show = 3\nfig,ax = plt.subplots(num_to_show,1,figsize=(7,7*num_to_show))\nfor i in range(num_to_show):\n    show_image(showobjects(denormalize_tensor(val_set[i][0]),val_set[i][1],val_set[i][2]),ax=ax[i])\nfig.tight_layout()\n_ = ax[0].set_title('Ground Truth')\n\n\n\n\n\ndef hw2corn(cntr:int,hw:int)-&gt;torch.Tensor:\n    return torch.cat([cntr-hw/2,cntr+hw/2],dim=1)\n\n\ndef corn2hw(y1, y2)-&gt;torch.Tensor:\n    return torch.cat([y1+y2/2, y2-y1], dim=1)\n\n\nmodel\n\ncreate_body?\n\nSignature: create_body(model, n_in=3, pretrained=True, cut=None)\nDocstring: Cut off the body of a typically pretrained `arch` as determined by `cut`\nFile:      ~/mambaforge/lib/python3.10/site-packages/fastai/vision/learner.py\nType:      function\n\n\n\nbody = create_body(resnet34(True))\nx, *_ = next(iter(train_dl))\nx.shape\n\n/home/hasan/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/home/hasan/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\ntorch.Size([16, 3, 224, 224])\n\n\n\nbody(x).shape\n\ntorch.Size([16, 512, 7, 7])\n\n\n\nour last layer is currently 7x7 shape\nwe will create some other feature map with 4x4, 2x2, 1x1\n\n\nclass ConvLayer(nn.Module):\n    def __init__(\n                self,\n                n_inp:int,\n                n_out:int,\n                stride:int=2,\n                drop:float=0.1):\n\n        super().__init__()\n        self.conv = nn.Conv2d(\n                             n_inp, \n                             n_out, \n                             kernel_size=3, \n                             stride=stride, \n                             padding=1)\n\n        self.bn = nn.BatchNorm2d(n_out)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        return self.drop(F.relu(self.bn(self.conv(x))))\n\n\ndef flat_conv(\n    x:torch.Tensor, \n    k:int, # feature map scaled with this factor\n    )-&gt;torch.Tensor:\n    \n\n    bs, n_f, h, w = x.size()\n    x = x.permute(0,2,3,1).contiguous() # [bs, n_f, h, w] -&gt; [bs, h, w, n_f]\n    return x.view(bs, -1, n_f//k)\n\n\nclass OutConv(nn.Module):\n    def __init__(\n                self, \n                k:int, # feature map scaled with this factor\n                n_inp:int, \n                bias:int):\n        super().__init__()\n        self.k = k\n        self.oconv1 = nn.Conv2d(n_inp, (len(vocab)*k), kernel_size=3, padding=1)\n        self.oconv2 = nn.Conv2d(n_inp, (4*k), kernel_size=3, padding=1)\n        self.oconv1.bias.data.zero_().add_(bias)\n\n    def forward(self, x):\n        return [flat_conv(self.oconv1(x), self.k), flat_conv(self.oconv2(x), self.k)]\n\n\n# Function to create a pattern of 45 overlapping anchor boxes in unit coordinates\n# and allow for scaling and translation\ndef mk_unit_anchors(scale,x,y,half=True):\n    a = []\n    # xyxy format!!\n    \n    l3 = 1.0\n    n3 = l3 * 0.33334\n    s3 = -n3\n    e3 = 1.+n3-l3\n\n    l5 = 0.5\n    n5 = l5 * 0.33334\n    s5 = -n5\n    e5 = 1.+n5-l5\n\n    # will create 9 square anchor boxes tiled in an overlapping 3x3 grid\n    xs = np.linspace(s3,e3,3)\n    ys = np.linspace(s3,e3,3)\n    a.extend([[y,x,y+l3,x+l3] for y in ys for x in xs])\n\n    if half:\n        # will create 18 rectangular boxes tiled in an overlapping 3x6 grid\n        # aspect ratio is 1:0.5\n        xs = np.linspace(s3,e3,3)\n        ys = np.linspace(s5,e5,6)\n        a.extend([[x,y,x+l3,y+l5] for y in ys for x in xs])\n\n        # will create 18 rectangular boxes tiled in an overlapping 6x3 grid\n        # aspect ratio is 0.5:1\n        xs = np.linspace(s5,e5,6)\n        ys = np.linspace(s3,e3,3)\n        a.extend([[x,y,x+l5,y+l3] for y in ys for x in xs])\n\n    return torch.clamp(tensor(a)*scale+tensor([x,y,x,y]),min=0,max=1)\n\n-Using the functions above, we’ll create a set of anchor boxes for each feature map and for each feature map location.\n\n# if image space is defined as 0.-1., then the following are the anchor box scales\n# for each feature map layer\ns7 = 0.125\ns4 = 0.25\ns2 = 0.5\ns1 = 1.0\n\na7 = len(mk_unit_anchors(s7,0,0))\na4 = len(mk_unit_anchors(s4,0,0))\na2 = len(mk_unit_anchors(s2,0,0))\na1 = len(mk_unit_anchors(s1,0,0))\n\nanchors_per_map_location = [a7,a4,a2,a1]\n\n# for each feature map 'location' create a set of anchor boxes (45)\nanchor_cnr = torch.cat([\n    torch.cat([mk_unit_anchors(s7,i*s7,j*s7) for i in range(7) for j in range(7)]),     # for 7x7x45\n    torch.cat([mk_unit_anchors(s4,i*s4,j*s4) for i in range(4) for j in range(4)]),     # for 4x4x45\n    torch.cat([mk_unit_anchors(s2,i*s2,j*s2) for i in range(2) for j in range(2)]),     # for 2x2x45\n    mk_unit_anchors(s1,0,0) # for 1x1x45\n    ]).to(device)\n\nanchors = corn2hw(anchor_cnr[:,:2], anchor_cnr[:,2:]).to(device)\n\nprint('The total number of anchor boxes is: ',len(anchors))\n\nThe total number of anchor boxes is:  3150\n\n\n\nclass SSD_Multihead(\n                    nn.Module):\n\n    def __init__(\n                self,\n                bias=-4,\n                printit=False,\n                drop=0\n                ):\n        super().__init__()\n        self.drop = nn.Dropout(drop)\n        \n\n\n        self.sconv0 = ConvLayer(\n                               512,\n                               256,\n                               stride=1, \n                               drop=drop) # 7x7\n    \n        self.sconv1 = ConvLayer(\n                               256,\n                               256,\n                               drop=drop) # 4x4 \n\n        self.sconv2 = ConvLayer(\n                               256,\n                               256,\n                               drop=drop) # 2x2 \n                               \n        self.sconv3 = ConvLayer(\n                               256,\n                               256,\n                               drop=drop) # 1x1\n\n        self.out0 = OutConv(a7, 256, bias)\n        self.out1 = OutConv(a4, 256, bias)\n        self.out2 = OutConv(a2, 256, bias)\n        self.out3 = OutConv(a1, 256, bias)\n        self.printit = printit\n\n\n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        if self.printit: print('sconv0:', x.shape)\n        o0c,o0l = self.out0(x)\n        if self.printit: print('out0:', o0l.shape)\n        x = self.sconv1(x)\n        if self.printit: print('sconv1:', x.shape)\n        o1c,o1l = self.out1(x)\n        if self.printit: print('out1:', o1l.shape)\n        x = self.sconv2(x)\n        if self.printit: print('sconv2:', x.shape)\n        o2c,o2l = self.out2(x)\n        if self.printit: print('out2:', o2l.shape)\n        x = self.sconv3(x)\n        if self.printit: print('sconv3:', x.shape)\n        o3c,o3l = self.out3(x)\n        if self.printit: print('out3:', o3l.shape)\n        return [torch.cat([o0c,o1c,o2c,o3c], dim=1), # 7x7, 4x4, 2x2, 1x1\n                torch.cat([o0l,o1l,o2l,o3l], dim=1)] # 7x7, 4x4, 2x2, 1x1\n\n\nhead_reg4 = SSD_Multihead(printit=True) # create an instance of custom head as described above\nbody = create_body(resnet34(True))  # create a resnet34 model with pretrained weights (downloaded) using fastai's create_body function\nmodel = nn.Sequential(body,head_reg4) # combine the body and the head into a single model\n\n/home/hasan/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/home/hasan/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\npred_class, pred_box = model(x)\n\nsconv0: torch.Size([16, 256, 7, 7])\nout0: torch.Size([16, 2205, 4])\nsconv1: torch.Size([16, 256, 4, 4])\nout1: torch.Size([16, 720, 4])\nsconv2: torch.Size([16, 256, 2, 2])\nout2: torch.Size([16, 180, 4])\nsconv3: torch.Size([16, 256, 1, 1])\nout3: torch.Size([16, 45, 4])\n\n\n\n\nloss function"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "detection_test",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "detection_test",
    "section": "Install",
    "text": "Install\npip install detection_test"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "detection_test",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  }
]